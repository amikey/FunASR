[//]: # (<div align="left"><img src="docs/images/funasr_logo.jpg" width="400"/></div>)

(ç®€ä½“ä¸­æ–‡|[English](./README.md))

# FunASR: A Fundamental End-to-End Speech Recognition Toolkit
<p align="left">
    <a href=""><img src="https://img.shields.io/badge/OS-Linux%2C%20Win%2C%20Mac-brightgreen.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/Python->=3.7,<=3.10-aff.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/Pytorch-%3E%3D1.11-blue"></a>
</p>

FunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼

<div align="center">  
<h4>
 <a href="#æ ¸å¿ƒåŠŸèƒ½"> æ ¸å¿ƒåŠŸèƒ½ </a>   
ï½œ<a href="#æœ€æ–°åŠ¨æ€"> æœ€æ–°åŠ¨æ€ </a>
ï½œ<a href="#å®‰è£…æ•™ç¨‹"> å®‰è£… </a>
ï½œ<a href="#å¿«é€Ÿå¼€å§‹"> å¿«é€Ÿå¼€å§‹ </a>
ï½œ<a href="https://alibaba-damo-academy.github.io/FunASR/en/index.html"> æ•™ç¨‹æ–‡æ¡£ </a>
ï½œ<a href="./docs/model_zoo/modelscope_models.md"> æ¨¡å‹ä»“åº“ </a>
ï½œ<a href="./funasr/runtime/readme_cn.md"> æœåŠ¡éƒ¨ç½² </a>
ï½œ<a href="#è”ç³»æˆ‘ä»¬"> è”ç³»æˆ‘ä»¬ </a>
</h4>
</div>

<a name="æ ¸å¿ƒåŠŸèƒ½"></a>
## æ ¸å¿ƒåŠŸèƒ½
- FunASRæ˜¯ä¸€ä¸ªåŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ï¼Œæä¾›å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆVADï¼‰ã€æ ‡ç‚¹æ¢å¤ã€è¯­è¨€æ¨¡å‹ã€è¯´è¯äººéªŒè¯ã€è¯´è¯äººåˆ†ç¦»å’Œå¤šäººå¯¹è¯è¯­éŸ³è¯†åˆ«ç­‰ã€‚FunASRæä¾›äº†ä¾¿æ·çš„è„šæœ¬å’Œæ•™ç¨‹ï¼Œæ”¯æŒé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ¨ç†ä¸å¾®è°ƒã€‚
- æˆ‘ä»¬åœ¨[ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition)ä¸[huggingface](https://huggingface.co/FunAudio)ä¸Šå‘å¸ƒäº†å¤§é‡å¼€æºæ•°æ®é›†æˆ–è€…æµ·é‡å·¥ä¸šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„[æ¨¡å‹ä»“åº“](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md)äº†è§£æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ã€‚ä»£è¡¨æ€§çš„[Paraformer](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹å…·æœ‰é«˜ç²¾åº¦ã€é«˜æ•ˆç‡ã€ä¾¿æ·éƒ¨ç½²çš„ä¼˜ç‚¹ï¼Œæ”¯æŒå¿«é€Ÿæ„å»ºè¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥é˜…è¯»([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](funasr/runtime/readme_cn.md))ã€‚

<a name="æœ€æ–°åŠ¨æ€"></a>
## æœ€æ–°åŠ¨æ€
- 20223/10/17: è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([ä¸€é”®éƒ¨ç½²æ–‡æ¡£](funasr/runtime/docs/SDK_tutorial_en_zh.md))
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€éŸ³è§†é¢‘è¯­æ–™åº“ï¼Œä¸»è¦æ˜¯åœ¨çº¿ä¼šè®®æˆ–è€…åœ¨çº¿è¯¾ç¨‹åœºæ™¯ï¼ŒåŒ…å«äº†å¤§é‡ä¸å‘è¨€äººè®²è¯å®æ—¶åŒæ­¥çš„å¹»ç¯ç‰‡ã€‚
- 2023.10.10: [Paraformer-long-Spk](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py)æ¨¡å‹å‘å¸ƒï¼Œæ”¯æŒåœ¨é•¿è¯­éŸ³è¯†åˆ«çš„åŸºç¡€ä¸Šè·å–æ¯å¥è¯çš„è¯´è¯äººæ ‡ç­¾ã€‚
- 2023.10.07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): FunCodecæä¾›å¼€æºæ¨¡å‹å’Œè®­ç»ƒå·¥å…·ï¼Œå¯ä»¥ç”¨äºéŸ³é¢‘ç¦»æ•£ç¼–ç ï¼Œä»¥åŠåŸºäºç¦»æ•£ç¼–ç çš„è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚
- 2023.09.01: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡2.0 CPUç‰ˆæœ¬å‘å¸ƒï¼Œæ–°å¢ffmpegã€æ—¶é—´æˆ³ä¸çƒ­è¯æ¨¡å‹æ”¯æŒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([ä¸€é”®éƒ¨ç½²æ–‡æ¡£](funasr/runtime/docs/SDK_tutorial_zh.md))
- 2023.08.07: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([ä¸€é”®éƒ¨ç½²æ–‡æ¡£](funasr/runtime/docs/SDK_tutorial_online_zh.md))
- 2023.07.17: BATä¸€ç§ä½å»¶è¿Ÿä½å†…å­˜æ¶ˆè€—çš„RNN-Tæ¨¡å‹å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[BAT](egs/aishell/bat)ï¼‰
- 2023.06.26: ASRU2023 å¤šé€šé“å¤šæ–¹ä¼šè®®è½¬å½•æŒ‘æˆ˜èµ›2.0å®Œæˆç«èµ›ç»“æœå…¬å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2_cn/index.html)ï¼‰

<a name="å®‰è£…æ•™ç¨‹"></a>
## å®‰è£…æ•™ç¨‹
FunASRå®‰è£…æ•™ç¨‹è¯·é˜…è¯»ï¼ˆ[Installation](https://alibaba-damo-academy.github.io/FunASR/en/installation/installation.html)ï¼‰

## æ¨¡å‹ä»“åº“

FunASRå¼€æºäº†å¤§é‡åœ¨å·¥ä¸šæ•°æ®ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨[æ¨¡å‹è®¸å¯åè®®](./MODEL_LICENSE)ä¸‹è‡ªç”±ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«FunASRæ¨¡å‹ï¼Œä¸‹é¢åˆ—ä¸¾ä»£è¡¨æ€§çš„æ¨¡å‹ï¼Œæ›´å¤šæ¨¡å‹è¯·å‚è€ƒ[æ¨¡å‹ä»“åº“]()


|                                                                                                         æ¨¡å‹åå­—                                                                                                         |        ä»»åŠ¡è¯¦æƒ…        |     è®­ç»ƒæ•°æ®     | å‚æ•°é‡  |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------:|:------------:|:----:|
| speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch ([ğŸ¤—]() [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary) ) |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |  60000å°æ—¶ï¼Œä¸­æ–‡  | 220M |
|                            speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/summary) )                            | åˆ†è§’è‰²è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶ |  60000å°æ—¶ï¼Œä¸­æ–‡  | 220M |
|           speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020 ([ğŸ¤—]() [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) )           |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |  50000å°æ—¶ï¼Œè‹±æ–‡  | 220M |
|                                 speech_conformer_asr-en-16k-vocab4199-pytorch ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) )                                 |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |  50000å°æ—¶ï¼Œè‹±æ–‡  | 220M |
|      speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) )                    |      è¯­éŸ³è¯†åˆ«ï¼Œå®æ—¶       |  60000å°æ—¶ï¼Œä¸­æ–‡  | 220M | 
|      punc_ct-transformer_cn-en-common-vocab471067-large ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) )                    |      æ ‡ç‚¹æ¢å¤ï¼Œéå®æ—¶      |  100Mï¼Œä¸­æ–‡ä¸è‹±æ–‡  | 1.1G | 
|      speech_fsmn_vad_zh-cn-8k-common ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-8k-common/summary) )                    |     è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼Œå®æ—¶      | 5000å°æ—¶ï¼Œä¸­æ–‡ä¸è‹±æ–‡ | 0.4M | 
|      speech_timestamp_prediction-v1-16k-offline ([ğŸ¤—]() [â­](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) )                    |      å­—çº§åˆ«æ—¶é—´æˆ³é¢„æµ‹      |  50000å°æ—¶ï¼Œä¸­æ–‡  | 38M  | 





<a name="å¿«é€Ÿå¼€å§‹"></a>
## å¿«é€Ÿå¼€å§‹
FunASRæ”¯æŒæ•°ä¸‡å°æ—¶å·¥ä¸šæ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„æ¨ç†å’Œå¾®è°ƒï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥å‚é˜…ï¼ˆ[modelscope_egs](https://alibaba-damo-academy.github.io/FunASR/en/modelscope_pipeline/quick_start.html)ï¼‰ï¼›ä¹Ÿæ”¯æŒå­¦æœ¯æ ‡å‡†æ•°æ®é›†æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥å‚é˜…ï¼ˆ[egs](https://alibaba-damo-academy.github.io/FunASR/en/academic_recipe/asr_recipe.html)ï¼‰ã€‚

ä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹
### step.1 åŠ è½½å¤´å®šä¹‰å’Œä¸‹è½½éŸ³é¢‘æ–‡ä»¶
```python
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

# ä¸­æ–‡æµ‹è¯•éŸ³é¢‘
wget https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav
# è‹±æ–‡æµ‹è¯•éŸ³é¢‘
wget 
```
### step.2 å®šä¹‰æ¨ç†pipeline

```python
inference_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch',
)
```
å…¶ä¸­ï¼Œ
- `task`å‚æ•°ä¸ºä»»åŠ¡ç±»å‹ï¼Œæ”¯æŒæœ‰ï¼šè¯­éŸ³è¯†åˆ«ï¼ˆ`Tasks.auto_speech_recognition`ï¼‰ï¼Œæ ‡ç‚¹æ¢å¤ï¼ˆ`Tasks.punctuation`ï¼‰ï¼Œè¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆ`Tasks.voice_activity_detection`ï¼‰ï¼Œæ—¶é—´æˆ³é¢„æµ‹ï¼ˆ`Tasks.speech_timestamp`ï¼‰ã€‚
- `model`å‚æ•°ä¸ºå…·ä½“æ¨¡å‹åï¼Œä¸`task`å‚æ•°é…åˆä½¿ç”¨ï¼Œæ”¯æŒ[æ¨¡å‹ä»“åº“]ä¸­ä»»æ„æ¨¡å‹ã€‚

### step.3 æ¨ç†éŸ³é¢‘
åˆ†ä¸ºéå®æ—¶æ¨¡å‹ä¸å®æ—¶æ¨¡å‹ï¼Œéå®æ—¶æ¨¡å‹è¾“å…¥ä¸ºæ•´å¥éŸ³é¢‘/æ–‡æœ¬ï¼Œå®æ—¶æ¨¡å‹è¾“å…¥ä¸ºéŸ³é¢‘æµæˆ–è€…ç‰‡æ®µ

#### éå®æ—¶æ¨¡å‹
```python
rec_result = inference_pipeline(audio_in='./vad_example.wav') #è¯­éŸ³è¾“å…¥
# rec_result = inference_pipeline(text_in='æˆ‘ä»¬éƒ½æ˜¯æœ¨å¤´äººä¸ä¼šè®²è¯ä¸ä¼šåŠ¨') #æ–‡æœ¬è¾“å…¥
```

#### å®æ—¶æ¨¡å‹
```python
import soundfile
speech, sample_rate = soundfile.read("./vad_example.wav")

chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms
encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention
decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention
param_dict = {"cache": dict(), "is_final": False, "chunk_size": chunk_size,
              "encoder_chunk_look_back": encoder_chunk_look_back, "decoder_chunk_look_back": decoder_chunk_look_back}
chunk_stride = chunk_size[1] * 960 # 600msã€480ms
# first chunk, 600ms
speech_chunk = speech[0:chunk_stride]
rec_result = inference_pipeline(audio_in=speech_chunk, param_dict=param_dict)
print(rec_result)
# next chunk, 600ms
speech_chunk = speech[chunk_stride:chunk_stride+chunk_stride]
rec_result = inference_pipeline(audio_in=speech_chunk, param_dict=param_dict)
print(rec_result)
```

æ›´å¤šè¯¦ç»†ç”¨æ³•ï¼ˆ[æ–°äººæ–‡æ¡£](https://alibaba-damo-academy.github.io/FunASR/en/funasr/quick_start_zh.html)ï¼‰


<a name="æœåŠ¡éƒ¨ç½²"></a>
## æœåŠ¡éƒ¨ç½²
FunASRæ”¯æŒé¢„è®­ç»ƒæˆ–è€…è¿›ä¸€æ­¥å¾®è°ƒçš„æ¨¡å‹è¿›è¡ŒæœåŠ¡éƒ¨ç½²ã€‚ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æœåŠ¡éƒ¨ç½²ï¼š

- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- ä¸­æ–‡æµå¼è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆGPUç‰ˆæœ¬ï¼‰ï¼Œè¿›è¡Œä¸­
- æ›´å¤šæ”¯æŒä¸­

è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚é˜…([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](funasr/runtime/readme_cn.md))ã€‚


<a name="ç¤¾åŒºäº¤æµ"></a>
## è”ç³»æˆ‘ä»¬

å¦‚æœæ‚¨åœ¨ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥åœ¨githubé¡µé¢æIssuesã€‚æ¬¢è¿è¯­éŸ³å…´è¶£çˆ±å¥½è€…æ‰«æä»¥ä¸‹çš„é’‰é’‰ç¾¤æˆ–è€…å¾®ä¿¡ç¾¤äºŒç»´ç åŠ å…¥ç¤¾åŒºç¾¤ï¼Œè¿›è¡Œäº¤æµå’Œè®¨è®ºã€‚

|                                  é’‰é’‰ç¾¤                                  |                          å¾®ä¿¡                           |
|:---------------------------------------------------------------------:|:-----------------------------------------------------:|
| <div align="left"><img src="docs/images/dingding.jpg" width="250"/>   | <img src="docs/images/wechat.png" width="215"/></div> |

## ç¤¾åŒºè´¡çŒ®è€…

| <div align="left"><img src="docs/images/nwpu.png" width="260"/> | <img src="docs/images/China_Telecom.png" width="200"/> </div>  | <img src="docs/images/RapidAI.png" width="200"/> </div> | <img src="docs/images/aihealthx.png" width="200"/> </div> | <img src="docs/images/XVERSE.png" width="250"/> </div> |
|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|

è´¡çŒ®è€…åå•è¯·å‚è€ƒï¼ˆ[è‡´è°¢åå•](./Acknowledge.md)ï¼‰


## è®¸å¯åè®®
é¡¹ç›®éµå¾ª[The MIT License](https://opensource.org/licenses/MIT)å¼€æºåè®®ï¼Œæ¨¡å‹è®¸å¯åè®®è¯·å‚è€ƒï¼ˆ[æ¨¡å‹åè®®](./MODEL_LICENSE)ï¼‰


## è®ºæ–‡å¼•ç”¨

``` bibtex
@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{An2023bat,
  author={Keyu An and Xian Shi and Shiliang Zhang},
  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{gao22b_interspeech,
  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},
  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2063--2067},
  doi={10.21437/Interspeech.2022-9996}
}
```
